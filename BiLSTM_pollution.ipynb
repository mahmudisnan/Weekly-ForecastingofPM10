{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 11:16:19.125477: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 11:16:19.134375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746504979.144135  940449 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746504979.146993  940449 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746504979.154659  940449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746504979.154668  940449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746504979.154669  940449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746504979.154670  940449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-06 11:16:19.157700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data, window_size):\n",
    "    \"\"\" Prepare sequences for BiLSTM training \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        seq_x = data[i:(i + window_size)]\n",
    "        seq_y = data[i + window_size]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buidl BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_bilstm(data, window_size):\n",
    "    # Scale 'casos' data for LSTM using MinMaxScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data['PM10 (ug/m3)'].values.reshape(-1,1))\n",
    "\n",
    "    # Prepare data for LSTM\n",
    "    X, y = prepare_sequences(scaled_data, window_size)\n",
    "\n",
    "    # Define LSTM model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(1000, activation='tanh'), input_shape=(window_size, 1)),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Fit the LSTM model\n",
    "    model.fit(X, y, epochs=150, batch_size=16, verbose=0)\n",
    "\n",
    "    # Initialize the results DataFrame\n",
    "    prediction_table = pd.DataFrame()\n",
    "\n",
    "    # Start the moving-window prediction\n",
    "    for i in range(len(data) - window_size):  # ensures space for predictions\n",
    "        # Prepare input sequence for prediction\n",
    "        input_seq = scaled_data[i:i + window_size]\n",
    "        input_seq = input_seq.reshape((1, window_size, 1))\n",
    "\n",
    "        # Predict the next 12 weeks\n",
    "        predictions = []\n",
    "        for weeks_ahead in [1, 2, 3, 4, 8, 12]:\n",
    "            future_index = i + window_size + weeks_ahead - 1\n",
    "            if future_index < len(data):\n",
    "                prediction = model.predict(input_seq, verbose=0)[0][0]\n",
    "                prediction = scaler.inverse_transform([[prediction]])[0][0]\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        new_row = {\n",
    "            'Datetime': data.iloc[i + window_size]['datetime'],\n",
    "            'PM10 (ug/m3)': data.iloc[i + window_size]['PM10 (ug/m3)'],\n",
    "            **{f'Predicted_Cases_Week{w}': p for w, p in zip([1, 2, 3, 4, 8, 12], predictions)}\n",
    "        }\n",
    "        prediction_table = pd.concat([prediction_table, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return prediction_table, model, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/Weekly PM10 in Andhra Pradesh.csv')\n",
    "data['datetime'] = pd.to_datetime(data['From Date'])\n",
    "window_size = 14  # Fixed window size\n",
    "results_df, model, scaler = predict_bilstm(data, window_size)\n",
    "print(results_df.head())\n",
    "\n",
    "def lag_predictions(df):\n",
    "    # Shift the prediction columns to introduce lags\n",
    "    df['Predicted_Cases_Week2'] = df['Predicted_Cases_Week2'].shift(1)  # Lag by 1\n",
    "    df['Predicted_Cases_Week3'] = df['Predicted_Cases_Week3'].shift(2)  # Lag by 2\n",
    "    df['Predicted_Cases_Week4'] = df['Predicted_Cases_Week4'].shift(3)  # Lag by 3\n",
    "    df['Predicted_Cases_Week8'] = df['Predicted_Cases_Week8'].shift(7)  # Lag by 7\n",
    "    df['Predicted_Cases_Week12'] = df['Predicted_Cases_Week12'].shift(11) # Lag by 11\n",
    "\n",
    "    return df\n",
    "\n",
    "# Use the function\n",
    "results_df = lag_predictions(results_df)\n",
    "print(results_df.head(20))  # Print the first 20 rows to see both original and lagged values\n",
    "\n",
    "results_df.to_csv('Output/BiLSTM_1000.csv', index=False)  # index=False means do not write row names (index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% uncertainty interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: /GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Detect computation device (GPU if available, otherwise CPU)\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to predict with uncertainty using MC Dropout (batch-based)\n",
    "@tf.function\n",
    "def predict_with_uncertainty_batch(model, input_seq, n_iter=500):\n",
    "    input_seq = tf.cast(input_seq, tf.float32)\n",
    "    input_batch = tf.repeat(input_seq, repeats=n_iter, axis=0)\n",
    "    preds = model(input_batch, training=True)\n",
    "    return tf.reduce_mean(preds, axis=0)\n",
    "\n",
    "# Main function for Adaptive Conformal Prediction (ACP)\n",
    "def adaptive_conformal_sliding(data, model, scaler, window_size, horizons=[1, 2, 3, 4, 8, 12],\n",
    "                               W=50, alpha=0.01, n_iter=500):\n",
    "    scaled_data = scaler.transform(data['PM10 (ug/m3)'].values.reshape(-1, 1))\n",
    "    results = []\n",
    "\n",
    "    for h in horizons:\n",
    "        covered = 0\n",
    "        total = 0\n",
    "\n",
    "        for t in range(window_size + h - 1 + W, len(data) - h):\n",
    "            # Calculate nonconformity scores in the previous window\n",
    "            window_scores = []\n",
    "            for w in range(t - W, t):\n",
    "                seq = scaled_data[w - window_size:w].reshape((1, window_size, 1))\n",
    "                mean_pred = predict_with_uncertainty_batch(model, seq, n_iter)\n",
    "                mean_pred = scaler.inverse_transform(np.array(mean_pred).reshape(-1, 1))[0][0]\n",
    "                true_val = data.iloc[w + h]['PM10 (ug/m3)']\n",
    "                score = abs(true_val - mean_pred)\n",
    "                window_scores.append(score)\n",
    "\n",
    "            # Calculate the (1 - alpha) quantile of the nonconformity scores\n",
    "            q_t = np.quantile(window_scores, 1 - alpha)\n",
    "\n",
    "            # Predict at time t using the current model\n",
    "            seq = scaled_data[t - window_size:t].reshape((1, window_size, 1))\n",
    "            mean_pred = predict_with_uncertainty_batch(model, seq, n_iter)\n",
    "            mean_pred = scaler.inverse_transform(np.array(mean_pred).reshape(-1, 1))[0][0]\n",
    "            lower = mean_pred - q_t\n",
    "            upper = mean_pred + q_t\n",
    "\n",
    "            true_val = data.iloc[t + h]['PM10 (ug/m3)']\n",
    "            if lower <= true_val <= upper:\n",
    "                covered += 1\n",
    "            total += 1\n",
    "\n",
    "        coverage = covered / total * 100 if total > 0 else 0\n",
    "        results.append({\n",
    "            'Week': h,\n",
    "            'Coverage_%': round(coverage, 2),\n",
    "            'Total': total,\n",
    "            'Covered': covered\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Uncertainty Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Week  Coverage_%  Total  Covered\n",
      "0     1       86.71    286      248\n",
      "1     2       89.79    284      255\n",
      "2     3       88.65    282      250\n",
      "3     4       87.14    280      244\n",
      "4     8       88.60    272      241\n",
      "5    12       87.12    264      230\n"
     ]
    }
   ],
   "source": [
    "acp_df = adaptive_conformal_sliding(data, model, scaler, window_size, W=50, alpha=0.1, n_iter=500)\n",
    "print(acp_df)\n",
    "acp_df.to_csv(\"Output/ACP_Coverage_Results_BiLSTMusingGraph.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(df):\n",
    "    errors = {}\n",
    "\n",
    "    for week in [1, 2, 3, 4, 8, 12]:\n",
    "        actual = df['PM10 (ug/m3)']\n",
    "        predicted = df[f'Predicted_pollution_Week{week}']\n",
    "\n",
    "        # Hilangkan NaN sebelum perhitungan error\n",
    "        valid_idx = ~actual.isna() & ~predicted.isna()\n",
    "        actual_valid = actual[valid_idx]\n",
    "        predicted_valid = predicted[valid_idx]\n",
    "\n",
    "        # Pastikan setidaknya ada satu data valid sebelum menghitung error\n",
    "        if len(actual_valid) > 0:\n",
    "            mae = mean_absolute_error(actual_valid, predicted_valid)\n",
    "            mse = mean_squared_error(actual_valid, predicted_valid)\n",
    "            rmse = mse ** 0.5\n",
    "            mape = (abs((actual_valid - predicted_valid) / actual_valid).mean()) * 100\n",
    "        else:\n",
    "            mae, mse, rmse, mape = None, None, None, None  # Jika tidak ada data valid, beri nilai None\n",
    "\n",
    "        errors[f'Week{week}'] = {'MAE': mae, 'MAPE': mape, 'RMSE': rmse, 'MSE': mse}\n",
    "\n",
    "\n",
    "    return errors\n",
    "\n",
    "# Hitung error\n",
    "errors = calculate_errors(results_df)\n",
    "\n",
    "\n",
    "errors_df = pd.DataFrame(errors).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Week1</th>\n",
       "      <td>346.633486</td>\n",
       "      <td>4.745286</td>\n",
       "      <td>499.229436</td>\n",
       "      <td>2.492300e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week2</th>\n",
       "      <td>2600.818296</td>\n",
       "      <td>43.838842</td>\n",
       "      <td>3460.205823</td>\n",
       "      <td>1.197302e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week3</th>\n",
       "      <td>3015.172559</td>\n",
       "      <td>55.513506</td>\n",
       "      <td>4086.557726</td>\n",
       "      <td>1.669995e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week4</th>\n",
       "      <td>3342.445354</td>\n",
       "      <td>63.321174</td>\n",
       "      <td>4498.051034</td>\n",
       "      <td>2.023246e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week8</th>\n",
       "      <td>3532.235120</td>\n",
       "      <td>56.010867</td>\n",
       "      <td>4593.070258</td>\n",
       "      <td>2.109629e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week12</th>\n",
       "      <td>3962.006605</td>\n",
       "      <td>67.016100</td>\n",
       "      <td>5033.233789</td>\n",
       "      <td>2.533344e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                MAE       MAPE         RMSE           MSE\n",
       "Week1    346.633486   4.745286   499.229436  2.492300e+05\n",
       "Week2   2600.818296  43.838842  3460.205823  1.197302e+07\n",
       "Week3   3015.172559  55.513506  4086.557726  1.669995e+07\n",
       "Week4   3342.445354  63.321174  4498.051034  2.023246e+07\n",
       "Week8   3532.235120  56.010867  4593.070258  2.109629e+07\n",
       "Week12  3962.006605  67.016100  5033.233789  2.533344e+07"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df.to_csv(\"Output/BiLSTM1000_150_03.csv\") #layer_epoch_dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
